{"paragraphs":[{"text":"%md\n# Hive Introduction\n\nThis document will act as a small example of how one can use the basic functionalities of Apache Hive.  It will discuss the various implications as to how to create and organize tables in order to increase preformance.\n\n## Importing Data to HDFS\n\nThe first step is to create a table.  The following HQL query below will create a new table using the data stored at `LOCATION`.  Note that this data is stored in gs which stands for Google Storage.  The data is external to the Hadoop cluster but it can now be accessed using a Hive table.","user":"anonymous","dateUpdated":"2019-08-20T18:56:44+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1566325461983_711265591","id":"20190820-182421_1353524930","dateCreated":"2019-08-20T18:24:21+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:235"},{"text":"%spark\nCREATE EXTERNAL TABLE wdi_gs (year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' LOCATION 'gs://[username]]/datasets/wdi_2016' TBLPROPERTIES (\"skip.header.line.count\"=\"1\");","user":"anonymous","dateUpdated":"2019-08-20T18:55:00+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1566315799588_1322442967","id":"20190820-154319_1869825787","dateCreated":"2019-08-20T15:43:19+0000","dateStarted":"2019-08-20T18:18:40+0000","dateFinished":"2019-08-20T18:18:40+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236"},{"text":"%md\n### Schema-on-read\n\nAn important thing to understand at this moment is the difference between schemas for HiveSQL and standard SQL. HiveSQL utilizes a schema-on-read approach to data, while SQL uses a schema-on-write. Schema-on-write defines how data in written to some storage container, which in this case is some SQL based RDBMS. In Hive, a schema defines how the data is read instead of how it is written. This introduces three new ideas which are not present in a RDBMS.\n\nFirst, a schema that describes how data is read requires data to already exist before reading. Unlike in an RDBMS where all the tables are created first, in Hive we want the data to already be available before creating a table. While it’s not required to have the data, we need to know what kind of data we will be looking at and where it will be stored.\n\nSecond, data does not need to be in any particular format (as long as it is in some structured format).  A Hive schema defines how data is read which means we have both the freedom and responsibility to tell Hive, via the schema, what kind of data format we are reading.  The schema will then translate the file format into a table structure as we read it.  In this case we are using a CSV file format which we tell to Hive using this line: `ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n'`.\n\nThird, more than one table can refer to a single source of data.  Since our schema defines how data is read we can give two or more definitions for a single file.  This provides flexibility in how we can examine our data.\n\nThe first idea is simple, and you'll see through this document that before any queries are executed that our tables have data to read from.  The second and third ideas are a little more complicated (but not much) and they will be explored more later on.\n\n### Create Command Description\nGetting back to our Hive command, we have defined a new table with columns and data types for those columns.  As mentioned earlier, the next part defines how the data is to be parsed by Hive, and finally we define the location of the data.  The last line is an extra flag that notifies Hive to skip the first line in the file.  In this case the data already exists so we can run some query immediately.","user":"anonymous","dateUpdated":"2019-08-20T18:55:49+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1566326216967_534760326","id":"20190820-183656_751834086","dateCreated":"2019-08-20T18:36:56+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:237"},{"text":"%spark\nSELECT count(*) FROM wdi_gs;","user":"anonymous","dateUpdated":"2019-08-20T18:59:27+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1566315820650_298540408","id":"20190820-154340_2080918579","dateCreated":"2019-08-20T15:43:40+0000","dateStarted":"2019-08-20T18:11:50+0000","dateFinished":"2019-08-20T18:11:51+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:238"},{"text":"%md\n### Pulling the Data\n\nUsing data that in external to Hadoop will slow down our executions as we need to wait for our whole data set to be transferred over the internet (or some network) to our internal workers nodes. In order to not waste time we will use Hive to copy the data from the first table to one that stored the data locally.\n\nFirst we create a new table:","user":"anonymous","dateUpdated":"2019-08-20T18:59:50+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1566327526575_2093344415","id":"20190820-185846_119218116","dateCreated":"2019-08-20T18:58:46+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:239"},{"text":"%spark\nCREATE EXTERNAL TABLE wdi_csv_text\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n'\nLOCATION 'hdfs:///user/[username]/hive/wdi/wdi_csv_text';","user":"anonymous","dateUpdated":"2019-08-20T19:01:06+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1566327590630_-511926201","id":"20190820-185950_730503391","dateCreated":"2019-08-20T18:59:50+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:240"},{"text":"%md\nThis command is almost identical to the first command with only two changed.  The table name is different since they need to be unique, and the location has been changed to a local directory in HDFS.  Next step is to copy the data from one table to the next:","user":"anonymous","dateUpdated":"2019-08-20T19:01:41+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1566327604355_2145071586","id":"20190820-190004_821335537","dateCreated":"2019-08-20T19:00:04+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:241"},{"text":"%spark\nINSERT OVERWRITE TABLE wdi_csv_text\nSELECT * FROM wdi_gs;","user":"anonymous","dateUpdated":"2019-08-20T19:02:02+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1566327702077_1096710556","id":"20190820-190142_549345768","dateCreated":"2019-08-20T19:01:42+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:242"},{"text":"%md\nAnd just to make sure, run a new query to check if the data is the same.","user":"anonymous","dateUpdated":"2019-08-20T19:05:46+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1566327887300_458902377","id":"20190820-190447_1186497289","dateCreated":"2019-08-20T19:04:47+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:243"},{"text":"%spark\nSELECT count(*) FROM wdi_gs_text;","user":"anonymous","dateUpdated":"2019-08-20T19:06:13+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1566327946214_-66055342","id":"20190820-190546_1126485796","dateCreated":"2019-08-20T19:05:46+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:244"},{"text":"%md\nNow that the data is local, we can examine it from outside of hive (assuming you have access to the machine running the namenode).  The following bash command will display the size of a table which can be use to check how effective the data was compressed.","user":"anonymous","dateUpdated":"2019-08-20T19:09:36+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1566327926659_-1574033744","id":"20190820-190526_608223596","dateCreated":"2019-08-20T19:05:26+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:245"},{"text":"%bash\nhdfs dfs -du -s -h hdfs:///user/[username]/hive/wdi/wdi_csv_text","user":"anonymous","dateUpdated":"2019-08-20T19:09:54+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1566328090476_1435257574","id":"20190820-190810_1992851008","dateCreated":"2019-08-20T19:08:10+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:246"},{"text":"%md\n## Fixing the Schema\nIf you have been exploring the data at all you might have realized that not everything is correct.  As it happens this is due to a problem with our schema.  We defined what a CSV is in a very naive way.  At every comma we start reading for a new column, and at every new line we start reading for a new row.  But what happens if we have commas or new lines as part of our column values?  In this case our data wouldn't be read properly.\n\nRun the following query and it will become apparent quickly that the schema has a flaw.","user":"anonymous","dateUpdated":"2019-08-20T19:14:45+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1566328202417_472384589","id":"20190820-191002_652594980","dateCreated":"2019-08-20T19:10:02+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:247"},{"text":"%spark\n","user":"anonymous","dateUpdated":"2019-08-20T19:14:51+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1566328485369_-154994406","id":"20190820-191445_1312644055","dateCreated":"2019-08-20T19:14:45+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:248"}],"name":"Hive Project","id":"2EMB9XCCH","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}